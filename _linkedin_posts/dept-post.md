---
title: "Dept Post"
date: "2026-02-06T14:50:36+00:00"
channel: "linkedin"
source_repo: "linkedin_posts"
source_path: "DEPT_Post.md"
external_url: ""
slug: "dept-post"
---

ğŸš¨ Our paper, DEPT: Decoupled Embeddings for Pre-Training Language Models, has been accepted for an Oral Presentation at hashtag#ICLR2025, taking place at the Singapore EXPO from April 24â€“28! ğŸ‡¸ğŸ‡¬ğŸ¤

This work is the result of a fantastic collaboration between University of Cambridge and Flower Labs, and we canâ€™t wait to share it with the community! ğŸ™Œ

ğŸ” What makes DEPT stand out?

âœ… Decoupled Embeddings: DEPT separates the embedding matrices from the transformer core, leading to a 4â€“5x reduction in memory usage
âœ… Vocabulary-Agnostic: Supports custom vocabularies across data sources without needing a shared tokenizer
âœ… Sync-Free Embeddings: Opens the door to efficient large-scale training in distributed or federated environments

ğŸ§  We see DEPT as a foundational shift in how we approach pre-training for language modelsâ€”enabling greater efficiency, flexibility, and scalability across distributed datasets.

ğŸ™ Special thanks to our incredible collaborators for their dedication in bringing DEPT to life! 
ğŸ“š DEPT Team: Alex Iacob, Lorenzo Sani, Meghdad Kurmanji, Bill Shen, Xinchi Qiu, Dongqi Cai, Yan Gao, Nicholas Lane

ğŸ”— Curious to learn more?
ğŸ“„ Paper: [https://lnkd.in/eDkR7rdC) 
ğŸ“ Blog post: [https://lnkd.in/eC7qaKbE)

ğŸ“¬ Questions or ideas?
Weâ€™d love to connectâ€”feel free to reach out!

ğŸ“ See you at ICLR 2025 in Singapore!
hashtag#ICLR2025 hashtag#DEPT hashtag#LLMs hashtag#FederatedLearning hashtag#AIResearch hashtag#CambridgeMLSystems hashtag#FlowerLabs
